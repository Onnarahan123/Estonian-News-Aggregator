import requests
from bs4 import BeautifulSoup
import json

def korja_uudised(url, allikas):
    uudised = []
    headers = {'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X)'}
    try:
        r = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(r.content, 'html.parser')
        
        # Otsime pealkirju (h1, h2, h3 on tavaliselt pealkirjad)
        for tag in soup.find_all(['h1', 'h2', 'h3', 'a'], limit=20):
            text = tag.get_text(strip=True)
            link = tag.get('href') if tag.name == 'a' else (tag.find('a').get('href') if tag.find('a') else None)
            
            # Filtreerime v채lja ainult "p채ris" uudised (peavad olema piisavalt pikad)
            if link and len(text) > 25:
                if not link.startswith('http'):
                    link = url.rstrip('/') + (link if link.startswith('/') else '/' + link)
                uudised.append({"allikas": allikas, "pealkiri": text, "link": link})
                if len(uudised) >= 5: break # V천tame igast allikast 5 t체kki
    except Exception as e:
        print(f"Viga {allikas}: {e}")
    return uudised

if __name__ == "__main__":
    koik = []
    koik.extend(korja_uudised("https://www.err.ee/", "ERR"))
    koik.extend(korja_uudised("https://www.delfi.ee/", "Delfi"))
    koik.extend(korja_uudised("https://www.postimees.ee/", "Postimees"))
    koik.extend(korja_uudised("https://eestinen.fi/", "Eestinen"))
    
    with open('uudised.json', 'w', encoding='utf-8') as f:
        json.dump(koik, f, ensure_ascii=False, indent=4)
    print(f"Valmis! Leiti {len(koik)} uudist.")
